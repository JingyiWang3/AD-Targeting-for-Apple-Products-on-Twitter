{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7887, 42)\n"
     ]
    }
   ],
   "source": [
    "# Import data from mongodb\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = pymongo.MongoClient(\"mongodb+srv://yh2866:Aa123456@cluster0-5mcg4.mongodb.net/tttest?retryWrites=true\")\n",
    "cursor = client['twitterdb']['AirPods']\n",
    "\n",
    "# Save to a dataframe\n",
    "twitterdf = pd.DataFrame(list(cursor.find()))\n",
    "print(twitterdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('config', \"InlineBackend.figure_format = 'retina'\")\n",
    "\n",
    "def get_entities(retweetNum,retweet,influenceDF):\n",
    "    \n",
    "    \"\"\"\n",
    "    input: \n",
    "        retweetNum: retweet count\n",
    "        retweet: retweetDF from get_retweet_df()\n",
    "        influenceDF: influenceDF  from get_retweet_df()\n",
    "        \n",
    "    output:\n",
    "         mydict:dictionary of nodes(entities) and edges( their relationships)in most-retweeted tweets\n",
    "         temp:list of index of malformatte tweets\n",
    "         G: netwrokx graph \n",
    "        \n",
    "    \"\"\"\n",
    "    nrow = np.sum(retweet.retweet_count > retweetNum)\n",
    "    print(nrow)\n",
    "    specialtweets= []\n",
    "    G = nx.Graph()\n",
    "    mydict = {'nodes':[],'links':[]}\n",
    "    for i in range(nrow):\n",
    "        amount = int(retweet.retweet_count.iloc[i])\n",
    "        temp = []\n",
    "        try:\n",
    "            tweet = retweet.text.iloc[i]\n",
    "            G.add_node(amount,color = \"lightblue\")# blue or green\n",
    "            dictionary = influenceDF.loc[influenceDF['text'].str.match(tweet),'entities'].any()\n",
    "            #weight = int(retweet.retweet_count.iloc[i])\n",
    "            mydict['nodes'].append({\"id\":amount,\"group\":1}) # tweets nodes :group1\n",
    "\n",
    "            for (key, value) in dictionary.items():\n",
    "                if value !=[]:\n",
    "                    if key in ['media','urls','symbols']:  \n",
    "                    \n",
    "                        [mydict['nodes'].append({\"id\":i['url'],\"group\":2}) for i in value] # url nodes :group2\n",
    "                        [mydict['links'].append({'source':amount,\\\n",
    "                                                 'target':i['url'],'value':1}) for i in value]# edge betweens tweets and url  \n",
    "                        temp.append([i['url'] for i in value])\n",
    "                        [G.add_node(i['url'],color = \"red\") for i in value]\n",
    "                        [G.add_edge(amount,j['url']) for j in value]\n",
    "              \n",
    "                    elif key in ['hashtags']:  #  hashtags nodes :group3 , yellow\n",
    "                        [mydict['nodes'].append({\"id\":'#' + i['text'] ,\"group\":3}) for i in value]\n",
    "                        [mydict['links'].append({'source':amount,'target':'#'+i['text'],'value':1}) for i in value]   \n",
    "                        temp.append(['#'+i['text'] for i in value])\n",
    "                        [G.add_node('#'+i['text'],color = \"yellow\") for i in value]\n",
    "                        [G.add_edge(amount,'#'+j['text']) for j in value]\n",
    "                    \n",
    "                    elif key in ['user_mentions']: # user mentioned nodes :group4 , orange\n",
    "                        [mydict['nodes'].append({\"id\":'@'+i['screen_name'],\"group\":4}) for i in value]\n",
    "                        [mydict['links'].append({'source':amount,'target':'@'+i['screen_name'],'value':1}) for i in value]\n",
    "                        temp.append(['@'+i['screen_name'] for i in value])\n",
    "                        [G.add_node('@'+i['screen_name'],color = \"orange\") for i in value]\n",
    "                        [G.add_edge(amount,'@'+j['screen_name']) for j in value]\n",
    "           # print(len(temp))\n",
    "            if len(temp) == 2:\n",
    "                for i in temp[0]:\n",
    "                    for j in temp[1]:\n",
    "                        mydict['links'].append({'source':i,'target':j,'value':1})\n",
    "                        G.add_edge(i,j)\n",
    "                         \n",
    "        except:\n",
    "            specialtweets.append(i)\n",
    "    print(G.number_of_nodes())\n",
    "    return mydict,specialtweets,G\n",
    "\n",
    "\n",
    "\n",
    "def draw_affinity_graph(df,name,retweetNum):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        df: raw product dataframe\n",
    "        name: name of the product/output graph \n",
    "        retweetNum: retweeted count\n",
    "    output:\n",
    "        networkx graph of entities and their relationships in most-retweeted tweets\n",
    "    \"\"\"\n",
    "\n",
    "    influenceDF,retweet = get_retweet_df(df)\n",
    "    mydict, temp ,G  = get_entities(retweetNum,retweet,influenceDF)\n",
    "    node_color = [G.node[node]['color'] for node in G]\n",
    "    pos=nx.spring_layout(G)\n",
    "    nx.draw(G, node_size = 100,node_color = node_color,            edge_color = \"grey\",with_labels = True,font_size = 10)\n",
    "    plt.gcf().set_size_inches(18.5, 10.5)\n",
    "    plt.savefig('../output/' + name + 'Affinity.png', dpi=300)\n",
    "    plt.show()\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAffinitygraph import draw_affinity_graph, get_entities,get_retweet_df\n",
    "influenceDF,retweet = get_retweet_df(twitterdf)\n",
    "mydict, specialtweets ,G  = get_entities(200,retweet,influenceDF)\n",
    "\n",
    "# draw_affinity_graph(AirPods_df,'Airpods',1000)\n",
    "\n",
    "\n",
    "import json\n",
    "def writeToJSONFile(path, fileName, data):\n",
    "     filePathNameWExt = './' + path + '/' + fileName + '.json'\n",
    "    with open(filePathNameWExt, 'w') as fp:\n",
    "         json.dump(data, fp)\n",
    "\n",
    "writeToJSONFile('./','affinityDictionary',mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niggas get AirPods and start acting like the ceo of Apple \n",
      "-------------\n",
      "I’m telling FAFSA y’all got AirPods \n",
      "-------------\n",
      "anyone:\n",
      "\n",
      "person with airpods: https://t.co/FLsrvuMsJd \n",
      "-------------\n",
      "Me: h-\n",
      "\n",
      "Person with AirPods: https://t.co/OvPs98l2lW \n",
      "-------------\n",
      "Last night it was cold &amp; windy &amp; when I opened my door this cricket hopped in, so I set him up good for the night:… https://t.co/PzmExaO9fY \n",
      "-------------\n",
      "nobody:\n",
      "\n",
      "person with airpods: https://t.co/FTQli5nS2r \n",
      "-------------\n",
      "nobody:\n",
      "\n",
      "person with airpods: https://t.co/FTQli5nS2r \n",
      "-------------\n",
      "Headphones guy vs. Airpods guy https://t.co/dah1I0cBlT \n",
      "-------------\n",
      "Who cares he has AirPods https://t.co/PPqLiHHLK7 \n",
      "-------------\n",
      "I ordered AirPods on amazon and got this shit. I AM FCKKNG MADDD. NAH BRUHHH SOMEONE IS LEAVING THIS EARTH TODAY https://t.co/a7g168YIR5 \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "specialtweets\n",
    "\n",
    "for i in range(len(specialtweets)):\n",
    "# [10, 19, 25, 27, 32, 38, 43, 46, 52, 55]\n",
    "#i = 10\n",
    "    tweet = retweet.text.iloc[i]\n",
    "    print(tweet,'\\n-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influenceDF.loc[influenceDF['text'].str.match(tweet),'entities'].any()\n",
    "# #weight = int(retweet.retweet_count.iloc[i])\n",
    "# mydict['nodes'].append({\"id\":amount,\"group\":1}) # tweets nodes :group1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most import nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Betweenness：你出现在连接任意两点最短路线的路上，你重要\n",
    "import operator\n",
    "bewnCent = nx.betweenness_centrality(G, normalized = True, endpoints = False)\n",
    "bewnCent = sorted(bewnCent.items(), key=operator.itemgetter(1),reverse = True)\n",
    "\n",
    "bewnCent_edge = nx.edge_betweenness_centrality(G, normalized = True)\n",
    "bewnCent_edge =  sorted(bewnCent_edge.items(), key=operator.itemgetter(1),reverse = True)\n",
    "# 你连接的 node 越多，你越重要\n",
    "degCent = nx.degree_centrality(G)\n",
    "degCent =  sorted(degCent.items(), key=operator.itemgetter(1),reverse = True)\n",
    "#：你距离里其它点的距离短，你越重要\n",
    "closeCent = nx.closeness_centrality(G, normalized = True)\n",
    "closeCent = sorted(closeCent.items(), key=operator.itemgetter(1),reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_symbols(df,entity):\n",
    "    textDF = pd.DataFrame()\n",
    "    j = 0\n",
    "    for row in range(df.shape[0]):\n",
    "        texts = df.iloc[row].entities.get(entity)\n",
    "        for i in range(len(texts)):\n",
    "            \n",
    "            text = texts[i]['text'] \n",
    "            textDF.loc[j, 'texts'] = text\n",
    "            textDF.loc[j, 'text'] = df.iloc[row].text\n",
    "            textDF.loc[j, 'index'] = row\n",
    "            j = j+1\n",
    "    textDF = textDF.astype({'index':'int'})\n",
    "    return textDF\n",
    "\n",
    "def get_url(df,entity): # 'media','urls',\n",
    "    urlDF = pd.DataFrame()\n",
    "    j = 0\n",
    "    for row in range(df.shape[0]):\n",
    "            urls = df.iloc[row].entities.get(entity)\n",
    "\n",
    "            for i in range(len(urls)):\n",
    "                url = urls[i]['url'] \n",
    "                urlDF.loc[j, 'url'] = url\n",
    "                urlDF.loc[j, 'text'] = df.iloc[row].text\n",
    "                urlDF.loc[j, 'index'] = row\n",
    "                j = j+1\n",
    "    urlDF = urlDF.astype({'index':'int'})\n",
    "    return urlDF\n",
    "\n",
    "\n",
    "def get_people(df):  # user_mentions screen_name \n",
    "    peopleDF = pd.DataFrame()\n",
    "    j = 0\n",
    "    for row in range(df.shape[0]):\n",
    "        people = df.iloc[row].entities.get('user_mentions')\n",
    "        for i in range(len(people)):\n",
    "            \n",
    "            name = people[i]['screen_name'] \n",
    "            peopleDF.loc[j, 'user_mentions'] = name\n",
    "            peopleDF.loc[j, 'text'] = df.iloc[row].text\n",
    "            peopleDF.loc[j, 'index'] = row\n",
    "            j = j+1\n",
    "    peopleDF = peopleDF.astype({'index':'int'})\n",
    "    return peopleDF\n",
    "\n",
    "\n",
    "\n",
    "# hashtagDF = get_hashtag_symbols(twitterdf,'hashtags')\n",
    "peopleDF = get_people(twitterdf)\n",
    "# urlDF =  get_url(twitterdf,'urls')\n",
    "# symbolsDF =  get_hashtag_symbols(twitterdf,'symbols')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peopleDF.groupby('user_mentions').agg('count').sort_values(by = 'text',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAffinitygraph import draw_affinity_graph, get_entities,get_retweet_df\n",
    "influenceDF,retweet = get_retweet_df(AirPods_df)\n",
    "mydict, temp ,G  = get_entities(1000,retweet,influenceDF)\n",
    "\n",
    "# draw_affinity_graph(AirPods_df,'Airpods',1000)\n",
    "\n",
    "\n",
    "import json\n",
    "def writeToJSONFile(path, fileName, data):\n",
    "     filePathNameWExt = './' + path + '/' + fileName + '.json'\n",
    "    with open(filePathNameWExt, 'w') as fp:\n",
    "         json.dump(data, fp)\n",
    "\n",
    "writeToJSONFile('./','affinityDictionary',mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
