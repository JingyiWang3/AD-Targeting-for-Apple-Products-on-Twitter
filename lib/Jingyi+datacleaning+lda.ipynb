{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7887, 42)\n"
     ]
    }
   ],
   "source": [
    "# Import data from mongodb\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = pymongo.MongoClient(\"mongodb+srv://yh2866:Aa123456@cluster0-5mcg4.mongodb.net/tttest?retryWrites=true\")\n",
    "cursor = client['twitterdb']['AirPods']\n",
    "\n",
    "# Save to a dataframe\n",
    "twitterdf = pd.DataFrame(list(cursor.find()))\n",
    "print(twitterdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hashtag_symbols(df,entity):\n",
    "    textDF = pd.DataFrame()\n",
    "    j = 0\n",
    "    for row in range(df.shape[0]):\n",
    "        texts = df.iloc[row].entities.get(entity)\n",
    "        for i in range(len(texts)):\n",
    "            \n",
    "            text = texts[i]['text'] \n",
    "            textDF.loc[j, 'texts'] = text\n",
    "            textDF.loc[j, 'text'] = df.iloc[row].text\n",
    "            textDF.loc[j, 'index'] = row\n",
    "            j = j+1\n",
    "    textDF = textDF.astype({'index':'int'})\n",
    "    return textDF\n",
    "\n",
    "def get_url(df,entity): # 'media','urls',\n",
    "    urlDF = pd.DataFrame()\n",
    "    j = 0\n",
    "    for row in range(df.shape[0]):\n",
    "            urls = df.iloc[row].entities.get(entity)\n",
    "\n",
    "            for i in range(len(urls)):\n",
    "                url = urls[i]['url'] \n",
    "                urlDF.loc[j, 'url'] = url\n",
    "                urlDF.loc[j, 'text'] = df.iloc[row].text\n",
    "                urlDF.loc[j, 'index'] = row\n",
    "                j = j+1\n",
    "    urlDF = urlDF.astype({'index':'int'})\n",
    "    return urlDF\n",
    "\n",
    "\n",
    "def get_people(df):  # user_mentions screen_name \n",
    "    peopleDF = pd.DataFrame()\n",
    "    j = 0\n",
    "    for row in range(df.shape[0]):\n",
    "        people = df.iloc[row].entities.get('user_mentions')\n",
    "        for i in range(len(people)):\n",
    "            \n",
    "            name = people[i]['screen_name'] \n",
    "            peopleDF.loc[j, 'user_mentions'] = name\n",
    "            peopleDF.loc[j, 'text'] = df.iloc[row].text\n",
    "            peopleDF.loc[j, 'index'] = row\n",
    "            j = j+1\n",
    "    peopleDF = peopleDF.astype({'index':'int'})\n",
    "    return peopleDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtagDF = get_hashtag_symbols(twitterdf,'hashtags')\n",
    "# peopleDF = get_people(twitterdf)\n",
    "# urlDF =  get_url(twitterdf,'urls')\n",
    "# symbolsDF =  get_hashtag_symbols(twitterdf,'symbols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from wordcloud import WordCloud\n",
    "dictionary = Dictionary(profile)\n",
    "\n",
    "vectors = [dictionary.doc2bow(text) for text in profile]\n",
    "vectors\n",
    "tfidf = TfidfModel(vectors)\n",
    "\n",
    "weights = tfidf[vectors[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# G.add_edges_from(\n",
    "#     [('A', 'B'), ('A', 'C'), ('D', 'B'), ('E', 'C'), ('E', 'F'),\n",
    "#      ('B', 'H'), ('B', 'G'), ('B', 'F'), ('C', 'G')])\n",
    "\n",
    "# val_map = {'A': 1.0,\n",
    "#            'D': 0.5714285714285714,\n",
    "#            'H': 0.0}\n",
    "\n",
    "#values = [val_map.get(node, 0.25) for node in G.nodes()]\n",
    "#\n",
    "#nx.draw(G, cmap=plt.get_cmap('jet'), node_color=values)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "node_color = [G.node[node]['color'] for node in G]\n",
    "\n",
    "\n",
    "cmap=plt.get_cmap('jet')\n",
    "\n",
    "nx.draw_networkx(G, node_color = node_color,with_labels = False,node_size  =10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAffinitygraph import draw_affinity_graph, get_entities,get_retweet_df\n",
    "influenceDF,retweet = get_retweet_df(AirPods_df)\n",
    "mydict, temp ,G  = get_entities(1000,retweet,influenceDF)\n",
    "# draw_affinity_graph(AirPods_df,'Airpods',1000)\n",
    "\n",
    "\n",
    "import json\n",
    "def writeToJSONFile(path, fileName, data):\n",
    "    filePathNameWExt = './' + path + '/' + fileName + '.json'\n",
    "    with open(filePathNameWExt, 'w') as fp:\n",
    "        json.dump(data, fp)\n",
    "\n",
    "writeToJSONFile('./','affinityDictionary',mydict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
